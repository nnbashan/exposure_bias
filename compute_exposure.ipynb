{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6a7d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reproduction script to calculate exposure .csv \n",
    "# that was used in the main file for figures\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import os \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from shapely.wkb import loads\n",
    "from datetime import timedelta\n",
    "from shapely.geometry import Polygon\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import ttest_ind\n",
    "from shapely import wkt\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b405ba-33e9-44b7-89c1-6a4eb34d1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "DATA_DIR  = Path(\"burada\")\n",
    "PAPER_DIR = Path(\"paper\")\n",
    "\n",
    "ZIP_SHP         = DATA_DIR / \"zip codes\" / \"Export_Output_5.shp\"\n",
    "MONITOR_CSV     = DATA_DIR / \"monitorloc.csv\"\n",
    "ID_MAP_CSV      = DATA_DIR / \"ids.csv\"\n",
    "WEIGHTS_CSV     = PAPER_DIR / \"bws_user_weights.csv\"\n",
    "QUALTRICS_CSV   = PAPER_DIR / \"qualtrics.csv\"\n",
    "AQ_DATA_CSV     = Path(\"aq_data.csv\")\n",
    "EPA_DAILY_CSV   = Path(\"daily_88101_2023.csv\")\n",
    "\n",
    "CRS_WGS84 = \"EPSG:4326\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Zip-code polygons\n",
    "# ---------------------------------------------------------------------\n",
    "zip_gdf = (\n",
    "    gpd.read_file(ZIP_SHP)\n",
    "       .set_crs(CRS_WGS84, allow_override=True)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Monitor locations \n",
    "# ---------------------------------------------------------------------\n",
    "monitor_df  = pd.read_csv(MONITOR_CSV)\n",
    "monitor_gdf = gpd.GeoDataFrame(\n",
    "    monitor_df,\n",
    "    geometry=gpd.points_from_xy(monitor_df[\"lon\"], monitor_df[\"lat\"]),\n",
    "    crs=CRS_WGS84,\n",
    ")\n",
    "\n",
    "# Clip monitors to zip-code extent\n",
    "monitor_gdf = gpd.clip(monitor_gdf, zip_gdf)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. ID mapping\n",
    "# ---------------------------------------------------------------------\n",
    "id_map = (\n",
    "    pd.read_csv(ID_MAP_CSV)\n",
    "      .rename(columns={\"ids1\": \"id\"})[[\"id\", \"ids2\"]]\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. User weights + demographics\n",
    "# ---------------------------------------------------------------------\n",
    "weights = (\n",
    "    pd.read_csv(WEIGHTS_CSV)\n",
    "      .rename(columns={\"mt_id\": \"user_id\"})\n",
    ")\n",
    "\n",
    "qualtrics = (\n",
    "    pd.read_csv(QUALTRICS_CSV)\n",
    "      .fillna(\"nan\")\n",
    "      .merge(id_map, how=\"left\")\n",
    "      .rename(columns={\"ids2\": \"user_id\"})\n",
    ")\n",
    "\n",
    "weights = weights.merge(qualtrics[[\"user_id\", \"occup\"]], how=\"left\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Air-quality datasets\n",
    "# ---------------------------------------------------------------------\n",
    "aq_data   = pd.read_csv(AQ_DATA_CSV)\n",
    "epa_daily = pd.read_csv(EPA_DAILY_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77aa5f8-7897-4d80-8ae7-883120c4eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Constants & paths\n",
    "# ---------------------------------------------------------------------\n",
    "DST_SWITCH_TS = pd.Timestamp(\"2023-11-05 07:00:00\")  # end of DST in 2023\n",
    "DATA_DIR      = Path(\"paper\")\n",
    "\n",
    "STAYS_CSV   = DATA_DIR / \"storylines_MAPC.csv\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Load stay-segments\n",
    "# ---------------------------------------------------------------------\n",
    "stays = pd.read_csv(\n",
    "    STAYS_CSV,\n",
    "    parse_dates=[\"started_at\", \"finished_at\"],   \n",
    ").sort_values(\"started_at\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Correct timestamps for DST gap (pre- vs. post-07:00 5 Nov 2023)\n",
    "# ---------------------------------------------------------------------\n",
    "dst_offset_hours = np.where(\n",
    "    stays[\"started_at\"] < DST_SWITCH_TS, 4, 5   # 4 h before switch, 5 h after\n",
    ")\n",
    "\n",
    "stays[\"corrected_time_start\"] = (\n",
    "    stays[\"started_at\"] - pd.to_timedelta(dst_offset_hours, unit=\"h\")\n",
    ")\n",
    "stays[\"corrected_time_end\"] = (\n",
    "    stays[\"finished_at\"] - pd.to_timedelta(\n",
    "        np.where(stays[\"started_at\"] < DST_SWITCH_TS, 4, 5), unit=\"h\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hour-of-day convenience columns\n",
    "stays[\"hour_start\"] = stays[\"corrected_time_start\"].dt.hour\n",
    "stays[\"hour_end\"]   = stays[\"corrected_time_end\"].dt.hour\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Keep users with ≥ 10 days between first & last corrected stay\n",
    "# ---------------------------------------------------------------------\n",
    "span_days = (\n",
    "    stays.groupby(\"user_id\")\n",
    "         .agg(span=(\"corrected_time_end\", lambda s: (s.max() - s.min()).days))\n",
    ")\n",
    "approved_users = span_days.query(\"span > 10\").index\n",
    "stays = stays[stays[\"user_id\"].isin(approved_users)]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Merge demographics, filter stay-only rows, and trim durations\n",
    "# ---------------------------------------------------------------------\n",
    "stays = (\n",
    "    stays.loc[stays[\"type\"] == \"Stay\"]          # keep only “Stay” rows\n",
    "         .merge(qualtrics[[\"user_id\", \"occup\"]], on=\"user_id\", how=\"left\")  # from earlier code\n",
    "         .query(\"30 < duration_min < 300\")      # duration 30–300 min\n",
    "         .copy()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Geometry handling\n",
    "# ---------------------------------------------------------------------\n",
    "stays[\"geometry\"] = [\n",
    "    loads(bytes.fromhex(wkb_hex)) for wkb_hex in stays[\"geometry\"]\n",
    "]\n",
    "stays_gdf = gpd.GeoDataFrame(stays, geometry=\"geometry\", crs=CRS_WGS84)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Extra temporal flags\n",
    "# ---------------------------------------------------------------------\n",
    "stays_gdf[\"day_start\"]   = stays_gdf[\"corrected_time_start\"].dt.day\n",
    "stays_gdf[\"month_start\"] = stays_gdf[\"corrected_time_start\"].dt.month\n",
    "stays_gdf[\"comb_dm\"]     = stays_gdf[\"month_start\"].astype(str) + \"/\" + stays_gdf[\"day_start\"].astype(str)\n",
    "\n",
    "stays_gdf[\"is_weekend\"] = np.where(\n",
    "    stays_gdf[\"corrected_time_start\"].dt.weekday >= 5, \"Yes\", \"No\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87cfae12-0d0d-4700-8104-6ec606c674e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAYPOINT_DIR = Path(\"burada/waypoints_folders\")\n",
    "LAT_COL      = \"latitude\"\n",
    "LON_COL      = \"longitude\"\n",
    "USER_COL     = \"user_id\"\n",
    "TIME_COL     = \"tracked_at\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Read & concatenate waypoint CSVs\n",
    "# ---------------------------------------------------------------------\n",
    "waypoint_dfs = [\n",
    "    pd.read_csv(csv_path) for csv_path in WAYPOINT_DIR.glob(\"*.csv\")\n",
    "]\n",
    "\n",
    "waypoints = pd.concat(waypoint_dfs, ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Clip to zip-code bounding box\n",
    "# (assumes `zcodes` — a GeoDataFrame — is already defined)\n",
    "# ---------------------------------------------------------------------\n",
    "min_lon, min_lat, max_lon, max_lat = zip_gdf.total_bounds  # xmin, ymin, xmax, ymax\n",
    "\n",
    "waypoints = waypoints.loc[\n",
    "    (waypoints[LAT_COL].between(min_lat, max_lat)) &\n",
    "    (waypoints[LON_COL].between(min_lon, max_lon))\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df69f9d-6790-4e67-ac54-0cb8aabd2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Calculate home locations\n",
    "# ---------------------------------------------------------------------\n",
    "import skmob\n",
    "from skmob.measures.individual import home_location\n",
    "traj = skmob.TrajDataFrame(waypoints,latitude=LAT_COL,longitude=LON_COL,user_id=USER_COL,datetime=TIME_COL)\n",
    "\n",
    "homes = home_location(traj, start_night=\"22:00\", end_night=\"06:00\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c816a07c-62e3-493c-a3f7-56e30cf62b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exposure-at-Stay computation\n",
    "\"\"\"\n",
    "homes = pd.read_csv('homes.csv')\n",
    "homes = gpd.GeoDataFrame(\n",
    "    homes, geometry=gpd.points_from_xy(homes.lng, homes.lat,crs = \"EPSG:4269\"))\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# CONSTANTS & PATHS\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "CRS_MERC  = \"EPSG:3857\"\n",
    "\n",
    "BUFFER_METERS = 4000  # radius for sensor buffers\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1.  Build buffered sensors & supporting look-ups\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def build_buffered_sensors(\n",
    "    sensors: gpd.GeoDataFrame,\n",
    "    stays: gpd.GeoDataFrame,\n",
    "    buf_m: int = BUFFER_METERS,\n",
    ") -> gpd.GeoDataFrame:\n",
    "\n",
    "    sensors_merc = sensors.to_crs(CRS_MERC)\n",
    "    buffers      = sensors_merc.buffer(buf_m).to_crs(CRS_WGS84)\n",
    "\n",
    "    buffered = gpd.GeoDataFrame(\n",
    "        sensors[[\"id\"]].copy(),\n",
    "        geometry=buffers,\n",
    "        crs=CRS_WGS84,\n",
    "    )\n",
    "\n",
    "    joined = gpd.sjoin(buffered, stays, how=\"inner\", predicate=\"intersects\")\n",
    "    return joined.reset_index(drop=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 2.  Per-stay exposure calculation\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def _calc_single_stay(\n",
    "    stay_idx: int,\n",
    "    stay_lookup: pd.Series,\n",
    "    buffered_sensors: gpd.GeoDataFrame,\n",
    "    aq_data: pd.DataFrame,\n",
    "    stays: gpd.GeoDataFrame,\n",
    "    buffered_homes: gpd.GeoDataFrame,\n",
    ") -> Tuple[List[int], List[float], List[float], List[str]]:\n",
    "  \n",
    "    \"\"\"\n",
    "    Compute exposure metrics for one stay, referenced by *stay_idx*.\n",
    "    \n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------------\n",
    "    # A.  Gather context for this stay\n",
    "    # -----------------------------------------------------------------\n",
    "    stay_id      = stay_lookup[stay_idx]                  \n",
    "    buf_slice    = buffered_sensors.query(\"index == @stay_id\")\n",
    "    sensor_ids   = buf_slice[\"id_left\"].tolist()\n",
    "    \n",
    "    user_id      = buf_slice[\"user_id\"].iat[0]\n",
    "    stay_row     = stays.loc[stays[\"id\"] == buf_slice[\"id_right\"].iat[0]].iloc[0]\n",
    "    \n",
    "    stays = stays_clipped.reset_index()\n",
    "    # Home / not-home label\n",
    "    label = (\n",
    "        \"home\"\n",
    "        if not buffered_homes.query(\"uid == @user_id\").sjoin(stays[stays['index']==stay_row.name][['geometry','id']]).empty\n",
    "        else \"not home\"\n",
    "    )\n",
    "    \n",
    "    start_t = stay_row[\"corrected_time_start\"]\n",
    "    end_t   = stay_row[\"corrected_time_end\"]\n",
    "    \n",
    "    day_mask = (aq_data[\"month\"] == start_t.month) & (aq_data[\"day\"] == start_t.day)\n",
    "    day_df   = aq_data.loc[day_mask].copy()\n",
    "    \n",
    "    daily_mean = (\n",
    "        day_df.groupby([\"year\", \"month\", \"day\"])[\"pms\"]\n",
    "        .transform(\"mean\")\n",
    "        .rename(\"daily_avg_pms\")\n",
    "    )\n",
    "    day_df[\"pms2\"] = day_df[\"pms\"] - daily_mean\n",
    "    \n",
    "    # Only rows from relevant sensors\n",
    "    sensor_df = day_df[day_df[\"ids\"].isin(sensor_ids)]\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # B.  30-minute sliding-window aggregation\n",
    "    # -----------------------------------------------------------------\n",
    "    idx_store, pms_raw_store, pms_det_store, label_store = [], [], [], []\n",
    "    \n",
    "    \n",
    "    sensor_df['lags2'] = pd.to_datetime(sensor_df['lags2'])\n",
    "    \n",
    "    if not sensor_df.empty:\n",
    "        win_end   = end_t\n",
    "        win_start = end_t - timedelta(minutes=30)\n",
    "    \n",
    "        while win_start > start_t:\n",
    "            window = sensor_df[\n",
    "                (sensor_df[\"lags2\"] > win_start) & (sensor_df[\"lags2\"] < win_end)\n",
    "            ]\n",
    "    \n",
    "            if not window.empty:\n",
    "                agg = (\n",
    "                    window[[\"month\", \"day\", \"hour\", \"minute\", \"pms\", \"pms2\"]]\n",
    "                    .groupby([\"month\", \"day\", \"hour\", \"minute\"])\n",
    "                    .median()\n",
    "                )\n",
    "    \n",
    "                pms_raw = agg[\"pms\"].mean()\n",
    "                pms_det = agg[\"pms2\"].mean()\n",
    "            else:\n",
    "                pms_raw = pms_det = -99.0\n",
    "    \n",
    "            # store\n",
    "            idx_store.append(stay_id)\n",
    "            pms_raw_store.append(pms_raw)\n",
    "            pms_det_store.append(pms_det)\n",
    "            label_store.append(label)\n",
    "    \n",
    "            # slide 30 min earlier\n",
    "            win_end   -= timedelta(minutes=30)\n",
    "            win_start -= timedelta(minutes=30)\n",
    "    else:\n",
    "        # No sensors in vicinity\n",
    "        idx_store.append(stay_id)\n",
    "        pms_raw_store.append(-99.0)\n",
    "        pms_det_store.append(-99.0)\n",
    "        label_store.append(label)\n",
    "    \n",
    "\n",
    "\n",
    "    return idx_store, pms_raw_store, pms_det_store, label_store\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3.  Wrapper for multiprocessing\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def run_parallel_exposure(\n",
    "    buffered_sensors: gpd.GeoDataFrame,\n",
    "    stays: gpd.GeoDataFrame,\n",
    "    df2t: pd.DataFrame,\n",
    "    buffered_homes: gpd.GeoDataFrame,\n",
    "    n_jobs: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Dispatch per-stay exposure computations in parallel\n",
    "    \"\"\"\n",
    "    indess       = buffered_sensors[\"index\"].unique()\n",
    "    stay_lookup  = pd.Series(indess)  \n",
    "\n",
    "    pool = mp.Pool(processes=n_jobs or mp.cpu_count())\n",
    "    try:\n",
    "        results = pool.starmap(\n",
    "            _calc_single_stay,\n",
    "            [\n",
    "                (\n",
    "                    idx,\n",
    "                    stay_lookup,\n",
    "                    buffered_sensors,\n",
    "                    aq_data,\n",
    "                    stays,\n",
    "                    buffered_homes,\n",
    "                )\n",
    "                for idx in range(len(stay_lookup))\n",
    "            ],\n",
    "        )\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Flatten the parallel lists\n",
    "    idx_all, pms_raw_all, pms_det_all, label_all = [], [], [], []\n",
    "    for ids_, raw_, det_, lab_ in results:\n",
    "        idx_all.extend(ids_)\n",
    "        pms_raw_all.extend(raw_)\n",
    "        pms_det_all.extend(det_)\n",
    "        label_all.extend(lab_)\n",
    "\n",
    "    exposures_raw = pd.DataFrame(\n",
    "        {\"index\": idx_all, \"exposure\": pms_raw_all, \"hm\": label_all}\n",
    "    )\n",
    "    exposures_det = pd.DataFrame(\n",
    "        {\"index\": idx_all, \"exposure\": pms_det_all, \"hm\": label_all}\n",
    "    )\n",
    "\n",
    "    return exposures_raw, exposures_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3262b483-ecf5-451e-a626-5368be41a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶  Starting exposure calculations: 2025-08-11 09:17:52\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Buffered home locations (200 m radius)\n",
    "# ---------------------------------------------------------------------\n",
    "BUFFER_HOME_M = 200  # metres\n",
    "\n",
    "buffered_homes = (\n",
    "    homes.to_crs(CRS_MERC)                   # project to metres\n",
    "         .buffer(BUFFER_HOME_M)              # create 200 m buffers\n",
    "         .to_crs(CRS_WGS84)                  # back to lat/long\n",
    "         .pipe(                              # wrap into a GeoDataFrame\n",
    "             lambda geom: gpd.GeoDataFrame(\n",
    "                 {\"uid\": homes[\"uid\"].values, \"geometry\": geom},\n",
    "                 crs=CRS_WGS84,\n",
    "             )\n",
    "         )\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"▶  Starting exposure calculations:\", datetime.now().isoformat(\" \", \"seconds\"))\n",
    "\n",
    "sensors_in_bounds = monitor_gdf[monitor_gdf[\"id\"].isin(np.unique(aq_data[\"ids\"]))]\n",
    "\n",
    "stays_clipped = gpd.clip(stays_gdf,zip_gdf)\n",
    "stays_clipped = stays_clipped.reset_index()\n",
    "\n",
    "buffered_sensors = (\n",
    "    build_buffered_sensors(sensors_in_bounds, stays_clipped)\n",
    "    .merge(weights[[\"user_id\", \"acs_weight\"]], on=\"user_id\", how=\"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac175f8e-5bb9-4468-9f2c-80a37809320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Done: 2025-08-11 09:29:34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stays_exp, stays_exp_detrended = run_parallel_exposure(\n",
    "    buffered_sensors,\n",
    "    stays_clipped, \n",
    "    aq_data,\n",
    "    buffered_homes,\n",
    ")\n",
    "\n",
    "print(\"✔  Done:\", datetime.now().isoformat(\" \", \"seconds\"))\n",
    "\n",
    "stays_exposure2=stays_exp_detrended.merge(stays_clipped)\n",
    "stays_exposure=stays_exp.merge(stays_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373f43f5-b1a7-45ca-8ab9-ef50f5cf079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_sensors = buffered_sensors[['id_left','geometry']].drop_duplicates('id_left')\n",
    "buffered_sensors = gpd.sjoin(buffered_sensors, homes, how=\"inner\")\n",
    "\n",
    "# 4. House-keeping objects used later in the script\n",
    "bufferedsensors_home = buffered_sensors.reset_index(drop=True)\n",
    "un_user              = bufferedsensors_home[\"uid\"].unique().tolist()\n",
    "work_ids             = buffered_homes[\"uid\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08f96a8-e4e2-4bdc-b32c-baa601d45ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. Per-stay exposure calculation (one process)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def _calc_stay_exposure(\n",
    "                    idx,\n",
    "                    stay_lookup,\n",
    "                    buffered_sensors,\n",
    "                    aq_data,\n",
    "                    stays,\n",
    "                    buffered_homes\n",
    ") -> Tuple[List[int], List[float], List[float], List[float]]:\n",
    "\n",
    "    stay_id   = stay_lookup[idx]\n",
    "    slice_    = buffered_sensors.query(\"index == @stay_id\")\n",
    "    sensor_ids = slice_[\"id_left\"].tolist()\n",
    "    uid        = slice_[\"user_id\"].iat[0]\n",
    "    \n",
    "    stay_geom  = stays_clipped.loc[stays_clipped[\"id\"] == slice_[\"id_right\"].iat[0]]\n",
    "    home_sensor_ids = buffered_homes.query(\"uid == @uid\")[\"id\"].unique()\n",
    "    \n",
    "    # Time bounds\n",
    "    start_t = slice_[\"corrected_time_start\"].iat[0]\n",
    "    end_t   = slice_[\"corrected_time_end\"].iat[0]\n",
    "    \n",
    "    # Extract same-day rows once\n",
    "    day_mask = (aq_data[\"month\"] == start_t.month) & (aq_data[\"day\"] == start_t.day)\n",
    "    day_df   = aq_data.loc[day_mask].copy()\n",
    "    \n",
    "    # Daily detrend\n",
    "    day_df[\"pms2\"] = day_df[\"pms\"] - day_df.groupby(\n",
    "        [\"year\", \"month\", \"day\"]\n",
    "    )[\"pms\"].transform(\"mean\")\n",
    "    \n",
    "    sensor_df      = day_df[day_df[\"ids\"].isin(sensor_ids)]\n",
    "    sensor_home_df = day_df[day_df[\"ids\"].isin(home_sensor_ids)]\n",
    "    \n",
    "    idxs, pms_raw, pms_det, pms_home = [], [], [], []\n",
    "    \n",
    "    if not sensor_df.empty:\n",
    "        sensor_df['lags2'] = pd.to_datetime(sensor_df['lags2'])\n",
    "        sensor_home_df['lags2'] = pd.to_datetime(sensor_home_df['lags2'])\n",
    "        win_end   = end_t\n",
    "        win_start = end_t - timedelta(minutes=30)\n",
    "    \n",
    "        while win_start > start_t:\n",
    "            w = sensor_df.query(\"@win_start < lags2 < @win_end\")\n",
    "            h = sensor_home_df.query(\"@win_start < lags2 < @win_end\")\n",
    "    \n",
    "            if not w.empty and not h.empty:\n",
    "                agg_w = (\n",
    "                    w.groupby([\"month\", \"day\", \"hour\", \"minute\"])\n",
    "                     .median()[[\"pms\", \"pms2\"]]\n",
    "                     .mean()\n",
    "                )\n",
    "                agg_h = (\n",
    "                    h.groupby([\"month\", \"day\", \"hour\", \"minute\"])\n",
    "                     .median()[\"pms\"]\n",
    "                     .mean()\n",
    "                )\n",
    "                p_raw = agg_w[\"pms\"]\n",
    "                p_det = agg_w[\"pms2\"]\n",
    "                p_home = agg_h\n",
    "            else:\n",
    "                p_raw = p_det = p_home = -99.0\n",
    "    \n",
    "            idxs.append(stay_id)\n",
    "            pms_raw.append(p_raw)\n",
    "            pms_det.append(p_det)\n",
    "            pms_home.append(p_home)\n",
    "    \n",
    "            win_end   -= timedelta(minutes=30)\n",
    "            win_start -= timedelta(minutes=30)\n",
    "    else:\n",
    "        idxs.append(stay_id)\n",
    "        pms_raw.append(pms_det.append(pms_home.append(-99.0)))\n",
    "\n",
    "    return idxs, pms_raw, pms_det, pms_home\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. Driver: parallel processing & final merge\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def compute_stay_exposure_home() -> gpd.GeoDataFrame:\n",
    "\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    try:\n",
    "        results = pool.starmap(\n",
    "            _calc_stay_exposure,\n",
    "            [\n",
    "                (\n",
    "                    idx,\n",
    "                    stay_lookup,\n",
    "                    buffered_sensors,\n",
    "                    aq_data,\n",
    "                    stays_clipped,\n",
    "                    buffered_homes,\n",
    "                )\n",
    "                for idx in range(len(stay_lookup))\n",
    "            ],\n",
    "        )\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # d.  COLLATE\n",
    "    idx_all, p_raw_all, p_det_all, p_home_all = [], [], [], []\n",
    "    for idxs, raws, dets, homes in results:\n",
    "        idx_all.extend(idxs)\n",
    "        p_raw_all.extend(raws)\n",
    "        p_det_all.extend(dets)\n",
    "        p_home_all.extend(homes)\n",
    "\n",
    "    exposure_df = pd.DataFrame(\n",
    "        {\"index_right\": idx_all,\n",
    "         \"exposure\":    p_raw_all,\n",
    "         \"home\":        p_home_all}\n",
    "    )\n",
    "\n",
    "    return exposure_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f551c0-7068-4356-86c5-04bf2ac66f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Start: 2025-08-11 09:29:35\n",
      "✔  Done: 2025-08-11 09:40:24\n"
     ]
    }
   ],
   "source": [
    "buffered_sensors = (\n",
    "    build_buffered_sensors(sensors_in_bounds, stays_clipped)\n",
    "    .merge(weights[[\"user_id\", \"acs_weight\"]], on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "indess       = buffered_sensors[\"index\"].unique()\n",
    "stay_lookup  = pd.Series(indess)  # simple positional index ➜ stay_id map\n",
    "\n",
    "BUFFER_HOME_M = 4000  # metres\n",
    "\n",
    "buffered_homes = (\n",
    "    homes.to_crs(CRS_MERC)                   # project to metres\n",
    "         .buffer(BUFFER_HOME_M)              # create 200 m buffers\n",
    "         .to_crs(CRS_WGS84)                  # back to lat/long\n",
    "         .pipe(                              # wrap into a GeoDataFrame\n",
    "             lambda geom: gpd.GeoDataFrame(\n",
    "                 {\"uid\": homes[\"uid\"].values, \"geometry\": geom},\n",
    "                 crs=CRS_WGS84,\n",
    "             )\n",
    "         )\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "buffered_homes = buffered_homes.sjoin(sensors_in_bounds[['id','geometry']])\n",
    "\n",
    "print(\"✔  Start:\", datetime.now().isoformat(\" \", \"seconds\"))\n",
    "\n",
    "stays_exposure_home = compute_stay_exposure_home()\n",
    "\n",
    "print(\"✔  Done:\", datetime.now().isoformat(\" \", \"seconds\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ec150e-0fa0-458c-a849-d768c788570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stays_exposure_home = stays_exposure_home.merge(stays_clipped,right_on='index',left_on='index_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94ece5-9655-4388-93c1-6dcd9fce993b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
